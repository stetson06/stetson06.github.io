---
layout: post
title: Retrieval Augmented Generation (RAG) LLM Chatbot for a Grocery Store
image: "/posts/grocery_tablet.png"
tags: [GenAI, RAG, LLM, ChatGPT, LangChain, Python]

---

This use case demonstrates the cutting-edge value of **Retrieval Augmented Generation (RAG)** AI models that leverage the large language models (LLMs) that we are all growing to know and love.

<br>

# Overview of Project

<br>

This discussion will first address why RAGs are necessary, given the incredible power of LLMs. It will then demonstrate how to create a RAG model that lives on top of an LLM (it will be ChatGPT-5 in this example), using the case of a fictional local grocery store.

<br>
<br>

# Limitations of Large Language Models (LLMs) and the Brilliance of the RAG Methodology

<br>

When a user interacts with an LLM and needs a specific answer for a particular store or business, they may get a decent answer, but that query can be inefficient. If the user's question requires a lot of uploaded documents into the prompt window, it may lead to a slow response (and may be expensive as well, if the account is pay-by-token processed), as we are asking the LLM to read everything fed to it (whether relevant or not), every time. For free accounts, loading large files into a prompt window to fill in any knowledge gaps could lead to the dreaded "truncation effect" - where the LLM cuts off the prompt at a certain amount of tokens (or words) and responds to effectively an incomplete prompt that has been cut off at a random spot. For most conventional LLMs not behind a paywall, that threshold is currently somewhere around 132K tokens. This has the potential to result in inaccurate responses (even if the loaded documents are current and the correct, most-relevant ones), depending on where the truncation occurs, often with the user unaware of what has just happened. Knowledge gaps also often lead to "hallucinations," or inaccurate responses from an LLM that don't reflect reality.

To address these inherent weaknesses in prompting, a new process, "retrieval augmented generation" (or "RAG"), has emerged that leverages (i.e., retrieves from) a database of entity-specific information (to include large numbers of relevant Question and Answer pairs, obtained from such sources as Q/A logs from call centers) and AI tools that identify which "chunks" of the database (i.e., Q/A pairs) are the most relevant to the user's prompt and feed (i.e., "augment") just that most useful information into the prompt window for "generation" of AI results. This clever technique very efficiently and accurately allows an informed LLM response without violating prompt window constraints - it is the brilliance of the RAG methodolgy and what is behind the sustained use of AI chatbot "assistants" commonly seen on business websites today.

<br>

# Task from ABC Grocery Store

<br>

Our fictional local business is the ABC Grocery Store. Their leaders have requested an online AI chatbot assistant to allow customers to ask ABC Grocery-specific questions on their company website and receive immediate, accurate responses. They have provided a list of 32 questions and answers in an .md text file (see below for sample). In real life, this list would be much more expansive and based on real-world help or call-center interactions with customers.

<br>
    ![yaml](/img/posts/rag_md_file.png)
<br>

# Development Methodology

<br>

This RAG project will utilize the following tools to provide ABC Grocery their requested AI chatbot:
1. **OpenAI's ChatGPT-5**
2. **LangChain's AI ecosystem platform**
3. **Python's LangChain library**

LangChain is a single ecosystem (i.e., framework) that has virtually all of the tools needed to build full-blown GenAI applications. It helps us take an LLM and connect it with data, tools, and workflows in a clean and modular way - it's known as the "glue that sticks all the pieces of an AI app together."

For both [OpenAI](https://platform.openai.com/docs/overview) and [LangChain](https://smith.langchain.com/), we first log-in and create the necessary API keys for each to enable Python connection to them. LangSmith is Langchain's MLOps layer (for support functions like debugging, evaluation/monitoring of the AI app). These keys are then copied onto an .env (Notepad) text file (see below) that is read by Python for the necessary API connections. 

<br>
    ![yaml](/img/posts/rag_env_file.png)
<br>

A .yaml text file, specified as below, is used to create the necessary virtual environment using the Anaconda prompt command shown below that:

<br>
    ![yaml](/img/posts/rag_yaml_file.png)
<br>

```
> cd (path working directory)
path working directory > conda env create -f gen-ai-virtual-env.yaml
path working directory > conda activate gen-ai
```                      

This virtual environment ("gen-ai") should now reside in Anaconda Navigator (drop-down menu at the top ribbon in the dialog box after "on") - this option should be activated each time we work on Python GenAI projects in the future.

<br>

# Python Code

With the virtual environment established, we proceed to coding in Python. Within the Spyder IDE, we ensure our working directory is pointing toward the folder containing the model's files. The first few sections of the code are administrative - they set up permissions and load the info file ('abc-grocery-help-desk-data.md') containing the Q/A pairs from ABC Grocery's notional Help Center.

```
# 01 - SET UP PERMISSIONS
from dotenv import load_dotenv
load_dotenv()

# 02 - LOAD DOCUMENT
from langchain_community.document_loaders import TextLoader

raw_filename = 'abc-grocery-help-desk-data.md'
loader = TextLoader(raw_filename, encoding="utf-8")
docs = loader.load()
print(docs)
text = docs[0].page_content
print(len(text))
print(text)
```
<br>

The "load_dotenv" module reads the API keys from our .env text file. This is much preferred to hard-coding keys into the code, which can compromise security. The "langchain_community.document_loader" [library](https://docs.langchain.com/oss/python/integrations/document_loaders) can read text-based documents (e.g., .txt, .md files) in a structured format. The class "TextLoader" loads the document ("docs") into memory and returns a list of LangChain document objects, each containing both the text concatenation and optional metadata, like source information (at the beginning of the output). The beginning of this very long string, to include the source info, is captured below:

<br>
    ![rag](/img/posts/rag_string.png)
<br>

This long text string, however, is not useful to us, so we ask the program to print the text in a delineated fashion using the "page_content" function (line "text = docs[0].page_content" - the 0 refers to the first and only item of this list). The last of the pairs displayed in the console is shown below:

<br>
    ![rag](/img/posts/rag_qa_32.png)
<br>

LangChain is super-flexible and has a ton of loader types that can be used to load not only text-based documents, but also CSV/PDF files, webpages, even things like loading content directly from X, Reddit, WhatsApp, Slack, etc. The parameter value encoding = "utf-8" ensures all text characters are correctly interpreted when the file is read.

The next section of code splits the document into chunks (i.e., 32 Q/A chunks).

<br>

```
# 03 - SPLIT DOCUMENT INTO CHUNKS
from langchain_text_splitters import MarkdownHeaderTextSplitter

splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[("###", "id")],
    strip_headers=True)

chunked_docs = splitter.split_text(text)
print(len(chunked_docs), "Q/A chunks")
print(chunked_docs[0])
print(chunked_docs[0].page_content)
```

<br>
    ![rag](/img/posts/rag_chunks.png)
<br>

The text has now been functionally split for independent relevance evaluation at the individual Q/A level, given a user's prompt. See here LangChain's documentation on its' splitters: [library](https://docs.langchain.com/oss/python/integrations/splitters).





**"How?"** 

*Natural* 


<br>  

# Old School NLP Technique
**Emerging Technolgies Hype Cycle** [publication](https://www.zdnet.com/article/gartner-releases-its-2021-emerging-tech-hype-cycle-heres-whats-in-and-headed-out/) :

| **Theme 1: Engineering Trust** | **Theme 2: Accelerating Growth** | **Theme 3: Sculpting Change** |
|:---|:---|:---|
|Sovereign Cloud|Generative AI|Composable Applications|
|Nonfungible Tokens (NFTs)|Digital Humans|Composable Networks|
|Machine-Readable Legislation|Multi-experience|AI-Augmented Software Engineering|
|Decentralized Identity (DCI)|Industry Cloud|AI-Augmented Design|
|Decentralized Finance (DeFi)|AI-Driven Innovation|Physics-Informed AI|
|Homomorphic Encryption|Quantum Machine Learning (Quantum ML)|Influence Engineering|
|Active Metadata Management||Digital Platform Conductor Tools|
|Data Fabric| |Named Data Networking (NDN)|
|Real-Time Incident Center| | |
|Employee Communications Applications| | |

<br>

<br>
    ![gartner](/img/posts/gartner.png)

<br>


<br>
**Generative AI: AI techniques that learn from existing artifacts to generate new, realistic content (images, text, audio, code) that reflects the characteristics of the training data but does not repeat it.**


```
python -m pip install python-docx
```                                                                     
We ran this command above in our Terminal (Mac/Linux) or Command Prompt/PowerShell (Windows) ("C:\Users\YOURACCOUNTNAME>") - if running Python through Anaconda, then use the Anaconda prompt and the command below:

```
conda install -c conda-forge python-docx
```
<br>  




```
pip install python-docx
```
<br>  

*GitHub, Inc.*

|Candidate Name |Emerging Technology |
|:---|:---|
|tech1|Nonfungible Tokens (NFTs)|
|tech2|Active Metadata Management |
|tech3|Generative AI|
|tech4|AI-Driven Innovation|
|tech5|Quantum Machine Learning (Quantum ML)|

<br>


```python

```

<br>

Important Limitation (Tables)
<br>

```python

```

<br>

Below is what the output looks like:
    ![strings](/img/posts/strings.png)
    
<br>

```python

```

The output is as below:
<br>
    ![angles](/img/posts/angles.png)
<br>

<br>
<br>
# State-of-the-Art Deep Learning/Artificial Neural Network Technique

*Vector Embeddings*

*sentence-transformers library* 
**BERT** 
*bidirectional encoder representations from transformers*

```python

```

<br>


```python

```

<br>
### How to Interpret the Output

| **Score** | **Alignment** | **Description** |
|:---:|:---|:---|
|0.80 - 1.00|Highly Aligned|The documents are likely talking about the exact same topics, perhaps even reusing the same paragraphs.|
|0.50 - 0.79|Moderately Aligned|They share the same context (e.g., both are about "Corporate Strategy" or "Q3 Goals"), but the specific content differs.|
| 0.20 - 0.49|Loosely Related|They might share a domain (e.g., "Business"), but the topics don't overlap much.|
|< 0.20|Unrelated|One is about Strategy, the other is likely about something completely different, like a lunch menu or unrelated IT logs.|

#### Important Note on Document Length

<br>
    ![angles2](/img/posts/angles2.png)
<br>

<br>
    ![pareto](/img/posts/pareto.png)
<br>

<br>

**which is the better method**

<br>
    ![oldvsnew](/img/posts/oldvsnew.png)

**meaning and context**.

<br>

# Growth & Next Steps

___








































































