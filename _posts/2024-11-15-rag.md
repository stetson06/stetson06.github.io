---
layout: post
title: Retrieval Augmented Generation (RAG) LLM Chatbot for a Grocery Store
image: "/posts/grocery_tablet.png"
tags: [GenAI, RAG, LLM, ChatGPT, LangChain, Python]

---

This use case demonstrates the cutting-edge value of Retrieval Augmented Generation (RAG) AI models that leverage the large language models (LLMs) that we are all growing to know and love.

<br>

# Overview of Project

<br>

This discussion will first address why RAGs are necessary, given the incredible power of LLMs. It will then demonstrate how to create a RAG model that lives on top of an LLM (it will be ChatGPT-5 in this case), using the case of a fictional local grocery store.

# Limitations of Large Language Models (LLMs)

<br>

Despite their extensive training, LLMs are generally unaware of proprietary information not available online. So asking an LLM for specific information (e.g., let's say specific personnel policies of our fictional ABC Grocery store that is not posted on their website) that is "unknowable" places LLMs that want to fully address prompts in a difficult position. In such a predicament, LLMs will reply with either an apology that it can't find the answer or worse, it will hallucinate an inaccurate response.

Conversely, loading large, potentially relevant documents into a prompt window to fill in the knowledge gap could lead to the dreaded "truncation effect" - where the LLM cuts off the prompt at a certain amount of tokens (or words) and responds to effectively an incomplete prompt that has been cut off at a random spot. For most conventional LLMs not behind a paywall, that threshold is currently somewhere around 132K tokens. This has the potential to result in inaccurate responses, depending on where the truncation occurs, often with the user unaware of what has just happened.

To address this inherent weakness, a new process has emerged that allows 


**"How?"** 

*Natural* 

*Artificial Neual Network*


[publication](https://platform.openai.com/docs/overview)


<br>  

# Old School NLP Technique
**Emerging Technolgies Hype Cycle** [publication](https://www.zdnet.com/article/gartner-releases-its-2021-emerging-tech-hype-cycle-heres-whats-in-and-headed-out/) :

| **Theme 1: Engineering Trust** | **Theme 2: Accelerating Growth** | **Theme 3: Sculpting Change** |
|:---|:---|:---|
|Sovereign Cloud|Generative AI|Composable Applications|
|Nonfungible Tokens (NFTs)|Digital Humans|Composable Networks|
|Machine-Readable Legislation|Multi-experience|AI-Augmented Software Engineering|
|Decentralized Identity (DCI)|Industry Cloud|AI-Augmented Design|
|Decentralized Finance (DeFi)|AI-Driven Innovation|Physics-Informed AI|
|Homomorphic Encryption|Quantum Machine Learning (Quantum ML)|Influence Engineering|
|Active Metadata Management||Digital Platform Conductor Tools|
|Data Fabric| |Named Data Networking (NDN)|
|Real-Time Incident Center| | |
|Employee Communications Applications| | |

<br>

<br>
    ![gartner](/img/posts/gartner.png)

<br>


<br>
**Generative AI: AI techniques that learn from existing artifacts to generate new, realistic content (images, text, audio, code) that reflects the characteristics of the training data but does not repeat it.**


```
python -m pip install python-docx
```                                                                     
We ran this command above in our Terminal (Mac/Linux) or Command Prompt/PowerShell (Windows) ("C:\Users\YOURACCOUNTNAME>") - if running Python through Anaconda, then use the Anaconda prompt and the command below:

```
conda install -c conda-forge python-docx
```
<br>  


```
pip install python-docx
```
<br>  

*GitHub, Inc.*

|Candidate Name |Emerging Technology |
|:---|:---|
|tech1|Nonfungible Tokens (NFTs)|
|tech2|Active Metadata Management |
|tech3|Generative AI|
|tech4|AI-Driven Innovation|
|tech5|Quantum Machine Learning (Quantum ML)|

<br>


```python

```

<br>

Important Limitation (Tables)
<br>

```python

```

<br>

Below is what the output looks like:
    ![strings](/img/posts/strings.png)
    
<br>

```python

```

The output is as below:
<br>
    ![angles](/img/posts/angles.png)
<br>

<br>
<br>
# State-of-the-Art Deep Learning/Artificial Neural Network Technique

*Vector Embeddings*

*sentence-transformers library* 
**BERT** 
*bidirectional encoder representations from transformers*

```python

```

<br>


```python

```

<br>
### How to Interpret the Output

| **Score** | **Alignment** | **Description** |
|:---:|:---|:---|
|0.80 - 1.00|Highly Aligned|The documents are likely talking about the exact same topics, perhaps even reusing the same paragraphs.|
|0.50 - 0.79|Moderately Aligned|They share the same context (e.g., both are about "Corporate Strategy" or "Q3 Goals"), but the specific content differs.|
| 0.20 - 0.49|Loosely Related|They might share a domain (e.g., "Business"), but the topics don't overlap much.|
|< 0.20|Unrelated|One is about Strategy, the other is likely about something completely different, like a lunch menu or unrelated IT logs.|

#### Important Note on Document Length

<br>
    ![angles2](/img/posts/angles2.png)
<br>

<br>
    ![pareto](/img/posts/pareto.png)
<br>

<br>

**which is the better method**

<br>
    ![oldvsnew](/img/posts/oldvsnew.png)

**meaning and context**.

<br>

# Growth & Next Steps

___



























































